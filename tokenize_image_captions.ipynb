{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Set env variable value from Google Colab secret\n"
      ],
      "metadata": {
        "id": "kSOtM-tkXbxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "GCP_PROJECT_ID = userdata.get('GCP_PROJECT_ID')\n",
        "\n",
        "os.environ[\"GCP_PROJECT_ID\"] = GCP_PROJECT_ID"
      ],
      "metadata": {
        "id": "4s8Ctf7YUQyj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Login to GCP project"
      ],
      "metadata": {
        "id": "dmWx_cV0XdMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBpt2ksbswa6"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login --no-launch-browser\n",
        "!gcloud config set project $GCP_PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download necessary input files from GCP bucket"
      ],
      "metadata": {
        "id": "hgC1nqO0XkRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!gsutil cp gs://virtual-home-studio/vhome_medium_ds/v0_t1680_images.tar.gz .\n",
        "!gsutil cp gs://virtual-home-studio/vhome_medium_ds/requirements.txt .\n",
        "!gsutil cp gs://virtual-home-studio/vhome_medium_ds/v0_t1680_image_2_captions.json .\n",
        "!tar -xzvf v0_t1680_images.tar.gz\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UA8FZcO5tCH9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatentate captions into single file"
      ],
      "metadata": {
        "id": "XWLLuVFGXSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def write_concatenated_captions_to_file(captions_dict, output_file):\n",
        "  \"\"\"\n",
        "  Writes all captions in a dictionary to a single text file.\n",
        "\n",
        "  Args:\n",
        "    captions_dict: A dictionary where keys are image filenames and values are CLIP generated image captions.\n",
        "    output_file: The path to the output text file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(output_file, 'w') as f:\n",
        "    for filename, caption in captions_dict.items():\n",
        "      # print(caption[0])\n",
        "      f.write(caption[0] + '. ')\n",
        "\n",
        "def read_captions_from_file(input_file):\n",
        "  \"\"\"\n",
        "  Read all captions in a dictionary to a single text file.\n",
        "\n",
        "  Args:\n",
        "    input_file: The path to the input text file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file) as f:\n",
        "      v0_t1680_image_2_captions = json.load(f)\n",
        "  return v0_t1680_image_2_captions\n",
        "\n",
        "\n",
        "v0_t1680_image_2_captions = read_captions_from_file('v0_t1680_image_2_captions.json')\n",
        "write_concatenated_captions_to_file(v0_t1680_image_2_captions, 'captions.txt')\n"
      ],
      "metadata": {
        "id": "4F3Oex3Et333"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preview captions generated from VirtualHome images."
      ],
      "metadata": {
        "id": "F2hZR5MkVnsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head captions.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tBQFLDazuW1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install depedencies"
      ],
      "metadata": {
        "id": "V04-85goXM2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kS90BGibysjW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Capture dependencies versions"
      ],
      "metadata": {
        "id": "IlJY2Bi_YYGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import re\n",
        "\n",
        "libs = [\"torch\", \"tiktoken\", \"tensorflow\", \"numpy\", \"pandas\", \"matplotlib\"]\n",
        "for lib in libs:\n",
        "    print(f\"{lib} version:\", version(lib))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnTEBbq1tsaU",
        "outputId": "9bc19980-d269-4110-aa3d-461102d19ae6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.1+cu121\n",
            "tiktoken version: 0.7.0\n",
            "tensorflow version: 2.15.0\n",
            "numpy version: 1.25.2\n",
            "pandas version: 2.2.2\n",
            "matplotlib version: 3.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a tokenizer for captions"
      ],
      "metadata": {
        "id": "yO1STUn1Yf1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"captions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    captions_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(captions_text))\n",
        "print(captions_text.split('.')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_I_-1s_15kX",
        "outputId": "db71841a-0411-4530-a990-420d0743437b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 81176\n",
            "a red refrigerator freezer sitting next to a sink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', captions_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(f'Total number of tokens present in corpus: {len(preprocessed)}')\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jJgcGOj2crZ",
        "outputId": "ccca35b2-b5ea-4e05-89dc-60cf04a4cc84"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens present in corpus: 19154\n",
            "['a', 'red', 'refrigerator', 'freezer', 'sitting', 'next', 'to', 'a', 'sink', '.', 'a', 'woman', 'standing', 'next', 'to', 'a', 'table', 'with', 'a', 'pie', 'on', 'it', '.', 'a', 'bathroom', 'with', 'a', 'sink', 'and', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(f'Vocab <> token ID: {item}')\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyJsrvOp2-JW",
        "outputId": "58276321-4935-4dba-ecc0-eb7c7de2386a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 243\n",
            "Vocab <> token ID: (\"'\", 0)\n",
            "Vocab <> token ID: (',', 1)\n",
            "Vocab <> token ID: ('.', 2)\n",
            "Vocab <> token ID: ('a', 3)\n",
            "Vocab <> token ID: ('above', 4)\n",
            "Vocab <> token ID: ('aerial', 5)\n",
            "Vocab <> token ID: ('against', 6)\n",
            "Vocab <> token ID: ('air', 7)\n",
            "Vocab <> token ID: ('an', 8)\n",
            "Vocab <> token ID: ('and', 9)\n",
            "Vocab <> token ID: ('another', 10)\n",
            "Vocab <> token ID: ('apartment', 11)\n",
            "Vocab <> token ID: ('are', 12)\n",
            "Vocab <> token ID: ('at', 13)\n",
            "Vocab <> token ID: ('back', 14)\n",
            "Vocab <> token ID: ('background', 15)\n",
            "Vocab <> token ID: ('backpack', 16)\n",
            "Vocab <> token ID: ('bath', 17)\n",
            "Vocab <> token ID: ('bathroom', 18)\n",
            "Vocab <> token ID: ('bathtub', 19)\n",
            "Vocab <> token ID: ('bed', 20)\n",
            "Vocab <> token ID: ('bedroom', 21)\n",
            "Vocab <> token ID: ('behind', 22)\n",
            "Vocab <> token ID: ('bench', 23)\n",
            "Vocab <> token ID: ('black', 24)\n",
            "Vocab <> token ID: ('blackboard', 25)\n",
            "Vocab <> token ID: ('blue', 26)\n",
            "Vocab <> token ID: ('book', 27)\n",
            "Vocab <> token ID: ('bookcase', 28)\n",
            "Vocab <> token ID: ('books', 29)\n",
            "Vocab <> token ID: ('bookshelf', 30)\n",
            "Vocab <> token ID: ('bookshelves', 31)\n",
            "Vocab <> token ID: ('boots', 32)\n",
            "Vocab <> token ID: ('bowl', 33)\n",
            "Vocab <> token ID: ('box', 34)\n",
            "Vocab <> token ID: ('boxes', 35)\n",
            "Vocab <> token ID: ('brown', 36)\n",
            "Vocab <> token ID: ('bunch', 37)\n",
            "Vocab <> token ID: ('cabinet', 38)\n",
            "Vocab <> token ID: ('cabinets', 39)\n",
            "Vocab <> token ID: ('can', 40)\n",
            "Vocab <> token ID: ('carpet', 41)\n",
            "Vocab <> token ID: ('ceiling', 42)\n",
            "Vocab <> token ID: ('cell', 43)\n",
            "Vocab <> token ID: ('chair', 44)\n",
            "Vocab <> token ID: ('chairs', 45)\n",
            "Vocab <> token ID: ('checkered', 46)\n",
            "Vocab <> token ID: ('clock', 47)\n",
            "Vocab <> token ID: ('close', 48)\n",
            "Vocab <> token ID: ('closet', 49)\n",
            "Vocab <> token ID: ('clothes', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizer uses encode method to tokenize texts into integers\n",
        "\n",
        "        Args:\n",
        "          text: A string containing the text to be tokenized.\n",
        "\n",
        "        Returns:\n",
        "          A list of integers representing the tokenized text\n",
        "        \"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Tokenizer uses decode method to convert integers back to text\n",
        "\n",
        "        Args:\n",
        "          ids: A list of integers representing the tokenized text.\n",
        "\n",
        "        Returns:\n",
        "          A string containing the decoded text.\n",
        "        \"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "tokenizer = TokenizerV1(vocab)\n",
        "\n",
        "text = \"a red refrigerator freezer sitting next to a sink\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(f\"Tokenized ids:\", ids)\n",
        "print(f\"Decoded text:\", tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fTw8rLP3st5",
        "outputId": "778591bc-0846-4bb9-cc53-73c8b83a5c49"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized ids: [3, 164, 165, 82, 182, 138, 209, 3, 180]\n",
            "Decoded text: a red refrigerator freezer sitting next to a sink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you spot any issues with this tokenizer? How would it handle words it hasn't seen before?"
      ],
      "metadata": {
        "id": "oIMwlnAKZZxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Support out of vocabulary words"
      ],
      "metadata": {
        "id": "RQzuoFRpZMBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "len(f'Number of tokens: {vocab.items()}')\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1HOn6M04IpT",
        "outputId": "da0c54b6-c75d-4cb6-95f8-4f594478168e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('women', 240)\n",
            "('wooden', 241)\n",
            "('yellow', 242)\n",
            "('<|endoftext|>', 243)\n",
            "('<|unk|>', 244)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizer uses encode method to tokenize texts into integers\n",
        "\n",
        "        Args:\n",
        "          text: A string containing the text to be tokenized.\n",
        "\n",
        "        Returns:\n",
        "          A list of integers representing the tokenized text\n",
        "        \"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Tokenizer uses decode method to convert integers back to text\n",
        "\n",
        "        Args:\n",
        "          ids: A list of integers representing the tokenized text.\n",
        "\n",
        "        Returns:\n",
        "          A string containing the decoded text.\n",
        "        \"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "sRL7hKNl4gFk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Test is a new string not in original vocabulary.\"\n",
        "text2 = \"a red refrigerator freezer sitting next to a sink\"\n",
        "\n",
        "\n",
        "text = \" <|EndOfText|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9keKVHJV4i23",
        "outputId": "6e043093-c6e3-4125-f9a5-29a2f0a21b28"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is a new string not in original vocabulary. <|EndOfText|> a red refrigerator freezer sitting next to a sink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMVq3cUL4wvG",
        "outputId": "6c200ae4-e319-4245-b2ad-23746166d7f9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[244,\n",
              " 107,\n",
              " 3,\n",
              " 244,\n",
              " 244,\n",
              " 244,\n",
              " 105,\n",
              " 244,\n",
              " 244,\n",
              " 2,\n",
              " 244,\n",
              " 3,\n",
              " 164,\n",
              " 165,\n",
              " 82,\n",
              " 182,\n",
              " 138,\n",
              " 209,\n",
              " 3,\n",
              " 180]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pmY_yFT44zTQ",
        "outputId": "d5fb9ffb-1408-407b-e6fd-0f2b65aaa797"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> is a <|unk|> <|unk|> <|unk|> in <|unk|> <|unk|>. <|unk|> a red refrigerator freezer sitting next to a sink'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see words not in the vocabulary be represented as `<|unk|>`."
      ],
      "metadata": {
        "id": "7AMR00aKbqMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare w/ BytePair encoding"
      ],
      "metadata": {
        "id": "WAIF8awhcIGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(captions_text)\n",
        "print(f\"Length of encoded text: {len(enc_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDjS-ah-SIfg",
        "outputId": "c97b3f43-6c15-48f4-9d61-b0d61b96c6a9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of encoded text: 19256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspect encoded text (context and next word)"
      ],
      "metadata": {
        "id": "MUvB45K8dERq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(f\"\"\"Token Id Context: {context}, \"---->\", Desired Id: {desired}\"\"\")"
      ],
      "metadata": {
        "id": "B9Q2cFK_SISN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(f\"\"\"Context: {tokenizer.decode(context)}, \"---->\", Desired: {tokenizer.decode([desired])}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_qQaGpVcfji",
        "outputId": "46600929-f1d1-4c9f-a23b-c1f52e600de4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:  television, \"---->\", Desired: .\n",
            "Context:  television., \"---->\", Desired:  a\n",
            "Context:  television. a, \"---->\", Desired:  living\n",
            "Context:  television. a living, \"---->\", Desired:  room\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create dataset and dataloader to extract chunks from the input text dataset\n"
      ],
      "metadata": {
        "id": "9oYc1h1ndU4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "mSkL6LfMSIGP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataloader with a batch size of 1 for an LLM with a context size of 4..."
      ],
      "metadata": {
        "id": "ihkR2of5n84J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    captions_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbceMWamSH60",
        "outputId": "7d345277-6308-4227-90f1-d04e269dc44d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   64,  2266, 30500, 30967]]), tensor([[ 2266, 30500, 30967,  5586]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(captions_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMvYzQAiS4fV",
        "outputId": "a58730f5-65a1-48f6-dacd-620414791255"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   64,  2266, 30500, 30967],\n",
            "        [ 5586,  1306,   284,   257],\n",
            "        [14595,    13,   257,  2415],\n",
            "        [ 5055,  1306,   284,   257],\n",
            "        [ 3084,   351,   257,  2508],\n",
            "        [  319,   340,    13,   257],\n",
            "        [12436,   351,   257, 14595],\n",
            "        [  290,   257, 16146,    13]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 2266, 30500, 30967,  5586],\n",
            "        [ 1306,   284,   257, 14595],\n",
            "        [   13,   257,  2415,  5055],\n",
            "        [ 1306,   284,   257,  3084],\n",
            "        [  351,   257,  2508,   319],\n",
            "        [  340,    13,   257, 12436],\n",
            "        [  351,   257, 14595,   290],\n",
            "        [  257, 16146,    13,   257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BytePair encoder has a vocabulary size of 50,257.\n",
        "If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector"
      ],
      "metadata": {
        "id": "1e28HDsDoQPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    captions_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "jRbp_yBCS6yV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdUVsdfXTNDl",
        "outputId": "28d4d5be-adb8-485d-be99-7f3f41ec0c53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   64,  2266, 30500, 30967],\n",
            "        [ 5586,  1306,   284,   257],\n",
            "        [14595,    13,   257,  2415],\n",
            "        [ 5055,  1306,   284,   257],\n",
            "        [ 3084,   351,   257,  2508],\n",
            "        [  319,   340,    13,   257],\n",
            "        [12436,   351,   257, 14595],\n",
            "        [  290,   257, 16146,    13]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73lb3oIsTNPj",
        "outputId": "039beba5-abb1-47c3-9d6b-49dadb532a1e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results in a 8 x 4 x 256 tensor since we have a batch size of 8 with 4 tokens\n"
      ],
      "metadata": {
        "id": "86qr56CqoxP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "qcjPRajTTNeA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 uses absolute position embeddings, so we just create another embedding layer...\n"
      ],
      "metadata": {
        "id": "LYnoW8i0pJWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm2fSgTNTSlx",
        "outputId": "ed30b11a-f362-4365-a1ce-f46416e4c1d6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the input embeddings used in an LLM, add the token and the positional embeddings..."
      ],
      "metadata": {
        "id": "2w090q4RpUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZijiAa_2TSwO",
        "outputId": "eafefd0f-232b-4a4d-cd24-5df0d8a9893d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "encoded_text = tokenizer.encode(captions_text)\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "context_length = 1024\n",
        "\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(captions_text, batch_size=8, max_length=max_length, stride=max_length)"
      ],
      "metadata": {
        "id": "hRlFlVjTtTra"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate and inspect input embeddings to sanity check..."
      ],
      "metadata": {
        "id": "qBMhje4Hp2eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "\n",
        "    token_embeddings = token_embedding_layer(x)\n",
        "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "\n",
        "    input_embeddings = token_embeddings + pos_embeddings\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "qTwYUAM4yRTM"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}